{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595b740b",
   "metadata": {},
   "source": [
    "## Mass Mobilization Research Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60647e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas pandas_gbq matplotlib networkx pytrends\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_gbq \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pytrends.request import TrendReq\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411e2baf",
   "metadata": {},
   "source": [
    "### GDelt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10b0ee4c",
   "metadata": {},
   "source": [
    "query = \"\"\"\n",
    "    SELECT MonthYear, count(*) FROM `gdelt-bq.full.events` WHERE EventBaseCode = \"014\" GROUP BY MonthYear ORDER BY MonthYear;\n",
    "\"\"\"\n",
    "\n",
    "event_df = pandas_gbq.read_gbq(query, project_id=\"gdelt-d-379201\")\n",
    "event_df.plot(x=\"MonthYear\", y=\"f0_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979b55cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=725825577420-unm2gnkiprugilg743tkbig250f4sfsj.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fbigquery&state=uRJjsKWIzrKShWK0FwdXZ4OcIvMCN1&access_type=offline\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * FROM `gdelt-bq.full.events` WHERE EventBaseCode = \"014\" LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "event_df = pandas_gbq.read_gbq(query, project_id=\"gdelt-d-379201\")\n",
    "event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b078b81",
   "metadata": {},
   "source": [
    "### Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3ef98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords(kw_list):\n",
    "    trends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    trends.build_payload(kw_list, timeframe='today 5-y')\n",
    "    kw_trend_df = trends.interest_over_time()\n",
    "    \n",
    "    return kw_trend_df\n",
    "    \n",
    "def add_keywords(new_kw_list, kw_trend_df):\n",
    "    new_kw_trend_df = load_keywords(new_kw_list)\n",
    "    \n",
    "    for col in new_kw_trend_df.columns:\n",
    "        kw_trend_df[col] = new_kw_trend_df[col]\n",
    "    \n",
    "    return kw_trend_df\n",
    "\n",
    "KEYWORDS = [\"blm\", \"defund\", \"abolish\", \"george floyd\", \"police\", \"prison\", \"protest\", \"justice\", \"trump\", \"all lives matter\", \"blm\", \"defund\", \"abolish\", \"police\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417fae22",
   "metadata": {},
   "source": [
    "### Load Keywords in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e689d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "chunks = len(KEYWORDS) // n\n",
    "kw_trend_df = pd.DataFrame()\n",
    "\n",
    "for i in range(chunks):\n",
    "    kw_chunk = KEYWORDS[(i * n):((i + 1) * n)]\n",
    "    kw_trend_df = add_keywords(kw_chunk, kw_trend_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc6b5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kw_trend_df.plot(y=[\"blm\", \"defund\", \"abolish\", \"police\"], use_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72513cc0",
   "metadata": {},
   "source": [
    "#### Plotting as a Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b1c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNIFICANCE_THRESHOLD = 0.3\n",
    "\n",
    "def generate_correlation_graph(trend_df, autocor=False, threshold=SIGNIFICANCE_THRESHOLD):\n",
    "    kw_list = list(trend_df.columns)\n",
    "    kw_list.remove(\"isPartial\")\n",
    "\n",
    "    kws = len(kw_list)\n",
    "    correlations = np.zeros((kws, kws))\n",
    "    kw_net = nx.Graph()\n",
    "    \n",
    "    for i in range(kws):\n",
    "        kw_net.add_node(i, label=kw_list[i])\n",
    "        \n",
    "        for j in range(kws):\n",
    "                if autocor or i != j:\n",
    "                    corr = kw_trend_df[kw_list[i]].corr(kw_trend_df[kw_list[j]])\n",
    "                    \n",
    "                    if np.isnan(corr):\n",
    "                        print(kw_list[i], kw_list[j])\n",
    "                    \n",
    "                    if corr != 0 and not np.isnan(corr):\n",
    "                        kw_net.add_edge(i, j, corr=corr, weight=np.abs(corr))\n",
    "        \n",
    "    return kw_net\n",
    "\n",
    "DEFAULT_WIDTH_FN = lambda edge: 3 * edge[\"corr\"] ** 2\n",
    "\n",
    "def choose_color(edge):\n",
    "    if edge[\"corr\"] > 0:\n",
    "        return \"lightgreen\"\n",
    "    else:\n",
    "        return \"red\"\n",
    "\n",
    "def draw(graph, layout=nx.spring_layout, width_fn=DEFAULT_WIDTH_FN):\n",
    "    pos = layout(graph)\n",
    "    edges = graph.edges()\n",
    "    weights = [width_fn(graph[i][j]) for i,j in edges]\n",
    "    edge_colors = [choose_color(graph[i][j]) for i,j in edges]\n",
    "    labels = nx.get_node_attributes(graph, \"label\")\n",
    "\n",
    "    nx.draw_networkx_labels(kw_net, pos, labels=labels)\n",
    "    nx.draw(\n",
    "        graph,\n",
    "        pos,\n",
    "        width=list(weights),\n",
    "        node_color=\"lightgrey\",\n",
    "        edge_color=edge_colors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa4875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kw_net = generate_correlation_graph(kw_trend_df, threshold=0)\n",
    "draw(kw_net, layout=nx.spring_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bcb3eb",
   "metadata": {},
   "source": [
    "### C-SPAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a9445",
   "metadata": {},
   "source": [
    "#### Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2aea649",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_CONGRESS = 97\n",
    "LAST_CONGRESS = 114\n",
    "congresses = range(FIRST_CONGRESS, LAST_CONGRESS + 1)\n",
    "\n",
    "def full_speeches_exist():\n",
    "    for congress in congresses:\n",
    "        if not os.path.isfile(f\"data/full/full_{get_num(congress)}.csv\"):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def get_num(congress):\n",
    "    if congress < 100:\n",
    "        return \"0\" + str(congress)\n",
    "    else:\n",
    "        return str(congress)\n",
    "\n",
    "def get_full_speeches():\n",
    "    for congress in congresses:\n",
    "        num = get_num(congress)\n",
    "\n",
    "        descsDf = pd.read_csv(f\"CongressData/descs/descr_{num}.txt\", sep=\"|\", encoding = 'unicode_escape', on_bad_lines='skip')\n",
    "        descsDf[\"speech_id\"] = descsDf[\"speech_id\"].astype(\"string\")\n",
    "\n",
    "        speechesDf = pd.read_csv(f\"CongressData/speeches/speeches_{num}.txt\", sep=\"|\", encoding = 'unicode_escape', on_bad_lines='skip')\n",
    "        speechesDf[\"speech_id\"] = speechesDf[\"speech_id\"].astype(\"string\")\n",
    "\n",
    "        speakersDf = pd.read_csv(f\"CongressData/speakers/{num}_SpeakerMap.txt\", sep=\"|\", encoding = 'unicode_escape', on_bad_lines='skip')\n",
    "        speakersDf[\"speech_id\"] = speechesDf[\"speech_id\"].astype(\"string\")\n",
    "\n",
    "        descsDf = descsDf.merge(speechesDf, on=\"speech_id\", how=\"outer\")\n",
    "        descsDf = descsDf.merge(speakersDf, on=\"speech_id\", how=\"outer\")\n",
    "\n",
    "        descsDf.to_csv(f\"CongressData/full/full_{num}.csv\")\n",
    "\n",
    "if not full_speeches_exist():\n",
    "    get_full_speeches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0363fdf",
   "metadata": {},
   "source": [
    "#### Converting C-SPAN to C-TRENDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525d0b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = get_num(FIRST_CONGRESS)\n",
    "\n",
    "descs = pd.read_csv(f\"CongressData/descs/descr_{num}.txt\", sep=\"|\", encoding = 'unicode_escape', on_bad_lines='skip', low_memory=False)\n",
    "full_speech = pd.read_csv(f\"CongressData/full/full_{num}.csv\", low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc996e3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['blm', 'defund', 'abolish', 'george floyd', 'police', 'prison', 'protest', 'justice', 'trump', 'all lives matter'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m     ctrends[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m to_datetime(ctrends[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ctrends\n\u001b[1;32m---> 30\u001b[0m ctrends \u001b[38;5;241m=\u001b[39m \u001b[43mget_ctrends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphrases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKEYWORDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mget_ctrends\u001b[1;34m(phrases, disag)\u001b[0m\n\u001b[0;32m     17\u001b[0m     ctrends_speech \u001b[38;5;241m=\u001b[39m full_speech\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mdisag])\u001b[38;5;241m.\u001b[39msum(numeric_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     18\u001b[0m     ctrends_speech \u001b[38;5;241m=\u001b[39m ctrends_speech\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphrase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: phrase \u001b[38;5;28;01mfor\u001b[39;00m phrase \u001b[38;5;129;01min\u001b[39;00m phrases})\n\u001b[1;32m---> 19\u001b[0m     ctrends_speech \u001b[38;5;241m=\u001b[39m \u001b[43mctrends_speech\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mphrases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdisag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m     ctrends \u001b[38;5;241m=\u001b[39m ctrends\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[0;32m     21\u001b[0m         ctrends_speech,\n\u001b[0;32m     22\u001b[0m         on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mdisag],\n\u001b[0;32m     23\u001b[0m         how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m     )\n\u001b[0;32m     26\u001b[0m ctrends[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m to_datetime(ctrends[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5876\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5874\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5876\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5878\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5880\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5937\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5938\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['blm', 'defund', 'abolish', 'george floyd', 'police', 'prison', 'protest', 'justice', 'trump', 'all lives matter'] not in index\""
     ]
    }
   ],
   "source": [
    "def to_datetime(col):\n",
    "    date = col.astype(\"str\").str\n",
    "    return pd.to_datetime(date[:4] + \"/\" + date[4:6] + \"/\" + date[6:])\n",
    "\n",
    "def get_ctrends(phrases=[], disag=[]):\n",
    "    ctrends = pd.DataFrame()\n",
    "\n",
    "    for congress in congresses:\n",
    "        num = get_num(congress)\n",
    "\n",
    "        full_speech = pd.read_csv(f\"CongressData/full/full_{num}.csv\", low_memory=False)\n",
    "        full_speech = full_speech[[*disag, \"speech\", \"date\"]]\n",
    "        \n",
    "        for phrase in phrases:\n",
    "            full_speech[f\"count_{phrase}\"] = full_speech[\"speech\"].str.count(phrase)\n",
    "        \n",
    "        ctrends_speech = full_speech.groupby([\"date\", *disag]).sum(numeric_only=True).reset_index()\n",
    "        ctrends_speech = ctrends_speech.rename({f\"count_{phrase}\": phrase for phrase in phrases})\n",
    "        ctrends_speech = ctrends_speech[[*phrases, *disag, \"date\"]]\n",
    "        ctrends = ctrends.merge(\n",
    "            ctrends_speech,\n",
    "            on=[\"date\", *disag],\n",
    "            how=\"cross\"\n",
    "        )\n",
    "    \n",
    "    ctrends[\"date\"] = to_datetime(ctrends[\"date\"])\n",
    "    \n",
    "    return ctrends\n",
    "\n",
    "ctrends = get_ctrends(phrases=KEYWORDS, disag=[\"party\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177f1e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
